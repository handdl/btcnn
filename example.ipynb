{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple, Dict\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from layers import (\n",
    "    BinaryTreeActivation,\n",
    "    BinaryTreeConv,\n",
    "    BinaryTreeInstanceNorm,\n",
    "    BinaryTreeAdaptivePooling,\n",
    ")\n",
    "from regressor import BinaryTreeSequential, BinaryTreeRegressor\n",
    "from torch import nn\n",
    "from torch.optim import lr_scheduler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://github.com/zinchse/hbo_bench/blob/main/dataset.py\n",
    "\n",
    "def paddify_sequences(sequences: \"List[Tensor]\", target_length: \"int\") -> \"List[Tensor]\":\n",
    "    \"\"\"\n",
    "    Pads sequences to make them of equal length.\n",
    "    \"\"\"\n",
    "    padded_sequences = []\n",
    "    n_channels = sequences[0].shape[1]\n",
    "    for seq in sequences:\n",
    "        padding_tokens = torch.zeros((target_length - len(seq), n_channels), dtype=seq.dtype, device=seq.device)\n",
    "        padded_seq = torch.cat((seq, padding_tokens), dim=0)\n",
    "        padded_sequences.append(padded_seq)\n",
    "    return padded_sequences\n",
    "\n",
    "\n",
    "class WeightedBinaryTreeDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        list_vertices: \"List[Tensor]\",\n",
    "        list_edges: \"List[Tensor]\",\n",
    "        list_time: \"List[Tensor]\",\n",
    "        device: \"torch.device\",\n",
    "    ):\n",
    "        \"\"\"\n",
    "        An iterator over <tensor of vectorized tree nodes, tree structure, frequency execution time>\n",
    "        with the ability to move data to the specified device.\n",
    "        \"\"\"\n",
    "        self.data_dict: \"Dict[Tuple, Dict]\" = {}\n",
    "\n",
    "        for vertices, edges, time in zip(list_vertices, list_edges, list_time):\n",
    "            key = str(vertices.flatten().tolist()), str(edges.flatten().tolist())\n",
    "            if key in self.data_dict:\n",
    "                self.data_dict[key][\"freq\"] += 1\n",
    "                self.data_dict[key][\"time\"].append(time)\n",
    "            else:\n",
    "                self.data_dict[key] = {\"vertices\": vertices, \"edges\": edges, \"time\": [time], \"freq\": 1}\n",
    "\n",
    "        self.list_vertices = [v[\"vertices\"] for v in self.data_dict.values()]\n",
    "        self.list_edges = [v[\"edges\"] for v in self.data_dict.values()]\n",
    "        self.list_time = [torch.stack(v[\"time\"]).mean() for v in self.data_dict.values()]\n",
    "        self.list_frequencies = [torch.tensor(v[\"freq\"]) for v in self.data_dict.values()]\n",
    "        self.size = len(self.data_dict)\n",
    "        self.device = device\n",
    "        self.move_to_device()\n",
    "\n",
    "    def move_to_device(self) -> \"None\":\n",
    "        for idx in range(self.size):\n",
    "            self.list_vertices[idx] = self.list_vertices[idx].to(device=self.device)\n",
    "            self.list_edges[idx] = self.list_edges[idx].to(device=self.device)\n",
    "            self.list_frequencies[idx] = self.list_frequencies[idx].to(device=self.device)\n",
    "            self.list_time[idx] = self.list_time[idx].to(device=self.device)\n",
    "\n",
    "    def __len__(self) -> \"int\":\n",
    "        return self.size\n",
    "\n",
    "    def __getitem__(self, idx) -> \"Tuple[Tensor, Tensor, Tensor, Tensor]\":\n",
    "        return self.list_vertices[idx], self.list_edges[idx], self.list_frequencies[idx], self.list_time[idx]\n",
    "\n",
    "def weighted_binary_tree_collate(\n",
    "    batch: \"List[Tuple[Tensor, Tensor, Tensor, Tensor]]\", target_length: \"int\"\n",
    ") -> \"Tuple[Tuple[Tensor, Tensor, Tensor], Tensor]\":\n",
    "    \"\"\"\n",
    "    Adds padding to equalize lengths, changes the number of axes and\n",
    "    their order to make neural network inference more suitable.\n",
    "    \"\"\"\n",
    "    list_vertices, list_edges, list_freq, list_time = [], [], [], []\n",
    "    for vertices, edges, freq, time in batch:\n",
    "        list_vertices.append(vertices)\n",
    "        list_edges.append(edges)\n",
    "        list_freq.append(freq)\n",
    "        list_time.append(time)\n",
    "\n",
    "    batch_vertices = torch.stack(paddify_sequences(list_vertices, target_length)).transpose(1, 2)\n",
    "    batch_edges = torch.stack(paddify_sequences(list_edges, target_length)).unsqueeze(1)\n",
    "    batch_freq = torch.stack(list_freq)\n",
    "    return (batch_vertices, batch_edges, batch_freq), torch.stack(list_time)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_node, l_child_node, ll_child_node, rl_child_node = (\n",
    "    [1.0, 1.0],\n",
    "    [1.0, -1.0],\n",
    "    [-1.0, -1.0],\n",
    "    [1.0, 1.0],\n",
    ")\n",
    "vertices = torch.tensor([root_node, l_child_node, ll_child_node, rl_child_node])\n",
    "\n",
    "padding_idx, root, l_child, ll_child, rl_child = (0, 1, 2, 3, 4)\n",
    "edges = torch.tensor(\n",
    "    [\n",
    "        [root, l_child, padding_idx],\n",
    "        [l_child, ll_child, rl_child],\n",
    "        [ll_child, padding_idx, padding_idx],\n",
    "        [rl_child, padding_idx, padding_idx],\n",
    "    ],\n",
    "    dtype=torch.long,\n",
    ")\n",
    "\n",
    "time = torch.tensor(42.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, dataset_size = 8, 8\n",
    "device = torch.device(\"cpu\")\n",
    "dataloader = DataLoader(\n",
    "    dataset=WeightedBinaryTreeDataset(\n",
    "        [vertices] * dataset_size, [edges] * dataset_size, [time] * dataset_size, device\n",
    "    ),\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda el: weighted_binary_tree_collate(el, 10),\n",
    "    drop_last=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building NN Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BinaryTreeRegressor(\n",
    "    btcnn=BinaryTreeSequential(\n",
    "        BinaryTreeConv(2, 128),\n",
    "        BinaryTreeInstanceNorm(128),\n",
    "        BinaryTreeActivation(torch.nn.functional.leaky_relu),\n",
    "        BinaryTreeAdaptivePooling(torch.nn.AdaptiveMaxPool1d(1))\n",
    "    ),\n",
    "    fcnn=nn.Sequential(\n",
    "        nn.Linear(128, 1),\n",
    "        nn.Softplus(),\n",
    "    ),\n",
    "    name=\"SimpleBTCNNRegressor\",\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_ckpt(\n",
    "    model: \"BinaryTreeRegressor\", optimizer: \"Optimizer\", scheduler: \"ReduceLROnPlateau\", epoch: \"int\", path: \"str\"\n",
    ") -> \"None\":\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"scheduler_state_dict\": scheduler.state_dict(),\n",
    "    }\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    torch.save(state, path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(model: \"BinaryTreeRegressor\", path: \"str\", device: \"torch.device\") -> \"BinaryTreeRegressor\":\n",
    "    ckpt_path = path\n",
    "    ckpt_state = torch.load(ckpt_path, map_location=device, weights_only=True)\n",
    "    model.load_state_dict(ckpt_state[\"model_state_dict\"])\n",
    "    model = model.to(device)\n",
    "    model.device = device\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: \"int\") -> \"None\":\n",
    "    torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_loss(\n",
    "    model: \"BinaryTreeRegressor\",\n",
    "    optimizer: \"Optimizer\",\n",
    "    criterion: \"nn.Module\",\n",
    "    dataloader: \"DataLoader\",\n",
    "    train_mode: \"bool\" = True,\n",
    ") -> \"float\":\n",
    "    _ = model.train() if train_mode else model.eval()\n",
    "    running_loss, total_samples = 0.0, 0\n",
    "    for (vertices, edges, freq), time in dataloader:\n",
    "        if train_mode:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(vertices, edges)\n",
    "        weighted_loss = (freq.float().squeeze(-1) * criterion(outputs.squeeze(-1), time)).mean()\n",
    "\n",
    "        if train_mode:\n",
    "            weighted_loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        running_loss += weighted_loss.item() * vertices.size(0)\n",
    "        total_samples += freq.sum()\n",
    "    return running_loss / total_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_train_loop(\n",
    "    model: \"BinaryTreeRegressor\",\n",
    "    optimizer: \"Optimizer\",\n",
    "    criterion: \"nn.Module\",\n",
    "    scheduler: \"ReduceLROnPlateau\",\n",
    "    train_dataloader: \"DataLoader\",\n",
    "    num_epochs: \"int\",\n",
    "    start_epoch: \"int\" = 0,\n",
    "    ckpt_period: \"int\" = 10,\n",
    "    path_to_save: \"Optional[str]\" = None,\n",
    ") -> \"None\":\n",
    "    tqdm_desc = \"Initialization\"\n",
    "    progress_bar = tqdm(range(start_epoch + 1, start_epoch + num_epochs + 1), desc=tqdm_desc, leave=True, position=0)\n",
    "    for epoch in progress_bar:\n",
    "        train_loss = calculate_loss(model, optimizer, criterion, train_dataloader)\n",
    "        scheduler.step(train_loss)\n",
    "        progress_bar.set_description(f\"[{epoch}/{start_epoch + num_epochs}] MSE: {train_loss:.4f}\")\n",
    "        if path_to_save and not epoch % ckpt_period:\n",
    "            save_ckpt(model, optimizer, scheduler, epoch, path_to_save)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[1000/1000] MSE: 0.0000: 100%|██████████| 1000/1000 [00:01<00:00, 858.52it/s]\n"
     ]
    }
   ],
   "source": [
    "lr, epochs = 3e-4, 1000\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=20)\n",
    "set_seed(42)\n",
    "\n",
    "weighted_train_loop(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.MSELoss(reduction=\"none\"),\n",
    "    scheduler=scheduler,\n",
    "    train_dataloader=dataloader,\n",
    "    num_epochs=epochs,\n",
    "    ckpt_period=epochs,\n",
    "    path_to_save=f\"/tmp/{model.name}.pth\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_loss = calculate_loss(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.MSELoss(reduction=\"none\"),\n",
    "    dataloader=dataloader,\n",
    "    train_mode=False,\n",
    ")\n",
    "assert final_loss < 1e-3, \"Problems with fitting\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = load_model(model, f\"/tmp/{model.name}.pth\", device)\n",
    "final_loss_after_reloading = calculate_loss(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    criterion=nn.MSELoss(reduction=\"none\"),\n",
    "    dataloader=dataloader,\n",
    "    train_mode=False,\n",
    ")\n",
    "assert abs(final_loss - final_loss_after_reloading) < 1e-3, \"Inconsistency after reloading\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
